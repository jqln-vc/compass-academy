#

||
|---|
|![Banner](/assets/banner-sprint9-desafio.png)|
||

## SE√á√ïES

* **Introdu√ß√£o ao Processamento de Linguagem Natural (NLP)** [÷ç](#introdu√ß√£o-ao-processamento-de-linguagem-natural-nlp)
  * **Tarefas de Pr√©-Processamento** [÷ç](#tarefas-de-pr√©-processamento)
  * **Vis√£o Geral do Hugging Face ü§ó** [÷ç](#vis√£o-geral-do-hugging-face-)
  * **Modelos Utilizados** [÷ç](#modelos-utilizados)
* **Camada Trusted Zone: Revisando os Dados** [÷ç](#camada-trusted-zone-revisando-os-dados)
  * **Primeiras Descobertas nos Dados** [÷ç](#primeiras-descobertas-nos-dados)
  * **Diagrama da Modelagem Dimensional** [÷ç](#diagrama-da-modelagem-dimensional)
* **Data Lake e Refined Zone** [÷ç](#data-lake-e-camada-refined-zone)
  * **Processamento de Dados: Etapa de Transforma√ß√£o Pt. 2** [÷ç](#processamento-de-dados-etapa-de-transforma√ß√£o-pt-2)
  * **An√°lise do Script do Glue Job** [÷ç](#an√°lise-do-script-do-glue-job)
    * **Importa√ß√µes** [÷ç](#importa√ß√µes)
    * **Vari√°veis** [÷ç](#vari√°veis)
    * **Modelos de L√≠ngua** [÷ç](#modelos-de-l√≠ngua)
    * **Cria√ß√£o dos DataFrames** [÷ç](#cria√ß√£o-dos-dataframes)
    * **Integra√ß√£o de Dados Entre Fontes Local e TMDB** [÷ç](#integra√ß√£o-de-dados-entre-fontes-local-e-tmdb)
    * **Fun√ß√µes Auxiliares, Extra√ß√£o de Dados e Cria√ß√£o de Novas Colunas** [÷ç](#fun√ß√µes-auxiliares-extra√ß√£o-de-dados-e-cria√ß√£o-de-novas-colunas)
      * **Categoriza√ß√£o de Regi√µes** [÷ç](#categoriza√ß√£o-de-regi√µes)
      * **Texto Total: T√≠tulo + Sinopse** [÷ç](#texto-total-t√≠tulo--sinopse)
      * **Detec√ß√£o de Conte√∫do Sexual e Sexismo** [÷ç](#detec√ß√£o-de-conte√∫do-sexual-e-sexismo)
      * **Classes Sint√°ticas e Termos Comuns** [÷ç](#classes-sint√°ticas-e-termos-comuns)
      * **Frequ√™ncia de Termos: Dicionariza√ß√£o** [÷ç](#frequ√™ncia-de-termos-dicionariza√ß√£o)
    * **Cria√ß√£o das Tabelas Dimens√£o, Fato e Bridge** [÷ç](#cria√ß√£o-das-tabelas-dimens√£o-fato-e-bridge)
      * **Cria√ß√£o Dimens√£o L√≠nguas** [÷ç](#cria√ß√£o-dimens√£o-l√≠nguas)
      * **Cria√ß√£o Dimens√£o Pa√≠ses** [÷ç](#cria√ß√£o-dimens√£o-pa√≠ses)
      * **Cria√ß√£o Dimens√£o T√≠tulos** [÷ç](#cria√ß√£o-dimens√£o-t√≠tulos)
      * **Cria√ß√£o Dimens√£o An√°lise Textual** [÷ç](#cria√ß√£o-dimens√£o-an√°lise-textual)
      * **Cria√ß√£o Dimens√£o Corpora** [÷ç](#cria√ß√£o-dimens√£o-corpora)
      * **Cria√ß√£o Dimens√£o Vocabul√°rio** [÷ç](#cria√ß√£o-dimens√£o-vocabul√°rio)
      * **Cria√ß√£o Fato Filmes**  [÷ç](#cria√ß√£o-fato-filmes)
      * **Cria√ß√£o Bridge Filmes-Vocabul√°rio** [÷ç](#cria√ß√£o-bridge-filmes-vocabul√°rio)
      * **Dele√ß√£o de Colunas de ID Auxiliares** [÷ç](#dele√ß√£o-de-colunas-de-id-auxiliares)
    * **Ingress√£o na Refined Zone** [÷ç](#ingress√£o-na-refined-zone)
* **Execu√ß√£o do Glue Job** [÷ç](#execu√ß√£o-do-glue-job)
  * **Cria√ß√£o e Execu√ß√£o do Crawler** [÷ç](#cria√ß√£o-e-execu√ß√£o-do-crawler)
* **Modelagem Dimensional: Vis√£o Geral com Athena** [÷ç](#modelagem-dimensional-vis√£o-geral-com-athena)
  * **Tabela Dimensional L√≠nguas** [÷ç](#tabela-dimensional-l√≠nguas)
  * **Tabela Dimensional Pa√≠ses** [÷ç](#tabela-dimensional-pa√≠ses)
  * **Tabela Dimensional T√≠tulos** [÷ç](#tabela-dimensional-t√≠tulos)
  * **Tabela Dimensional An√°lise Textual** [÷ç](#tabela-dimensional-an√°lise-textual)
  * **Tabela Dimensional Vocabul√°rio** [÷ç](#tabela-dimensional-vocabul√°rio)
  * **Tabela Dimensional Corpora** [÷ç](#tabela-dimensional-corpora)
  * **Tabela Fato Filmes** [÷ç](#tabela-fato-filmes)
  * **Tabela Bridge Filmes-Vocabul√°rio** [÷ç](#tabela-bridge-filmes-vocabul√°rio)
* **Vis√£o Geral do Bucket Dramance** [÷ç](#vis√£o-geral-do-bucket-dramance)
* **Considera√ß√µes Finais: Dificuldades e Pontos de Melhoria** [÷ç](#considera√ß√µes-finais-dificuldades-e-pontos-de-melhoria)
* **Refer√™ncias** [÷ç](#refer√™ncias)

## INTRODU√á√ÉO AO PROCESSAMENTO DE LINGUAGEM NATURAL (NLP)

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Uma sub√°rea da Intelig√™ncia Artificial, o Processamento de Linguagem Natural (ou NLP) engloba todas as t√©cnicas de compreens√£o e tratamento de dados da l√≠ngua humana (escrita ou oral). A complexidade da comunica√ß√£o pela l√≠ngua se estende para al√©m do texto em si, dependente de referenciais externos, vis√µes da realidade e da percep√ß√£o compartilhadas pelos seres humanos, e subentendidas durante as intera√ß√µes pela l√≠ngua.

Al√©m disso, as maneiras de codificar a l√≠ngua humana n√£o s√£o padronizadas, lidar com um dicion√°rio ou um artigo acad√™mico √© muito diferente de lidar com coment√°rios em redes sociais, onde a l√≠ngua assume formas org√¢nicas e din√¢micas de significado, com neologismos, s√≠mbolos, emojis, jogos de palavras, trunca√ß√µes, abrevia√ß√µes, etc.

Apesar das especificidades, existem etapas gerais de processamento de textos j√°, mais ou menos, consolidadas em rotinas (*pipelines*) de f√°cil acesso por meio de diversos pacotes. Nesta se√ß√£o, ser√£o introduzidos alguns conceitos e ferramentas para trabalhar com NLP.

### TAREFAS DE PR√â-PROCESSAMENTO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Antes de come√ßar a aplicar fun√ß√µes anal√≠ticas em um texto, √© preciso process√°-lo de forma a facilitar a separa√ß√£o de ocorr√™ncias relevantes, limpeza de ru√≠dos e uniformiza√ß√£o de formas distintas com significados similares; s√£o tratamentos diversos com fins de otimizar a identifica√ß√£o de padr√µes significativos.

> ***Normaliza√ß√£o** de palavras √© a tarefa de colocar palavras, ou **tokens**, em um formato padr√£o. O caso mais simples de normaliza√ß√£o de palavras √© o **case folding**. Mapear tudo para min√∫sculas [...] √© muito √∫til para generaliza√ß√£o em muitas tarefas, tais como recupera√ß√£o de informa√ß√µes ou reconhecimento de fala. Para an√°lise de sentimento e outras tarefas de classifica√ß√£o [...], a caixa da letra pode ser muito √∫til, e o **case folding** geralmente n√£o √© realizado.* [^1]

A caracteriza√ß√£o do que √© mantido ou desprezado depende da tarefa em quest√£o, no entanto, os processos em si s√£o rotineiros; a seguir alguns relevantes, brevemente comentados para a compreens√£o das etapas de tratamento dos dados de texto:

* **Parsing e Tokeniza√ß√£o**
  
O tratamento de tokeniza√ß√£o das strings das colunas de texto foi realizado, quase inteiramente, por **express√µes regulares**, primeiramente com split baseado em espa√ßos, limpeza de caracteres especiais e consolida√ß√£o de termos de interesse em uma string de tokens separados por v√≠rgulas. Posteriormente, esses tokens foram separados a partir das v√≠rgulas.

> *O **parsing, ou an√°lise sint√°tica**, √© necess√°rio quando a string cont√©m mais do que texto simples. Por exemplo, se o texto bruto vem de uma p√°gina web, email ou algum log, ent√£o cont√©m alguma estrutura adicional. [...] Depois de um parsing superficial, essa por√ß√£o de texto simples do documento pode passar pelo processo de **tokeniza√ß√£o**. Assim, tornando a string - uma sequ√™ncia de caracteres - em uma sequ√™ncia de **tokens**. Cada token pode, ent√£o, ser contabilizado como uma palavra. O tokenizador precisa saber quais caracteres indicam que um token terminou e outro est√° come√ßando. Espa√ßos e pontua√ß√£o s√£o, geralmente bons separadores.* [^2]

* **Remo√ß√£o de Stopwords**

Os **tokens** gerados foram filtrados para desprezar temos n√£o-alfab√©ticos e ***stopwords***, que n√£o agregam significado lexical √† an√°lise.

> *Classifica√ß√£o e recupera√ß√£o de dados geralmente n√£o necessitam uma compreens√£o aprofundada do texto. Por exemplo, na frase "Emma bateu em uma porta", a palavras "em" e "uma" n√£o alteram o fato de que essa frase √© sobre uma pessoa e uma porta. Para tarefas menos granulares, como classifica√ß√£o, os pronomes, artigos e preposi√ß√µes n√£o acrescentam muito valor. O caso pode ser muito diferente na an√°lise de sentimentos, a qual requer uma compreens√£o mais refinada de sem√¢ntica.* [^3]

* **Lematiza√ß√£o**
  
Dentre os tokens filtrados na etapa anterior, s√£o armazenados somente termos das classes sint√°ticas de substantivos, adjetivos, verbos e adv√©rbios. Tais termos s√£o convertidos para sua forma "lemma"; uma forma normalizada que despreza afixos, permitindo agregar ocorr√™ncias de um mesmo significado, que estejam flexionadas ou conjugadas de diversas maneiras. No vocabul√°rio, as palavras s√£o inseridas e contabilizadas na tabela bridge Filmes-Vocab somente ap√≥s a lematiza√ß√£o.

> *Lematiza√ß√£o √© a tarefa de determinar que 2 palavras possuem a mesma ra√≠z, apesar de suas diferen√ßas aparentes. As palavras "am", "are" e "is" possuem em comum o mesmo **lemma** "be"; as palavras "dinner" e "dinners" o mesmo **lemma** "dinner". [...] Os m√©todos mais sofisticados para a lematiza√ß√£o envolvem uma an√°lise morfossint√°tica (**parsing**) completa da palavra. [...] Morfologia √© o estudo do morfema, o modo que as palavras s√£o constru√≠das a partir de partes menores, contendo significados. Duas grandes classes de morfemas podem ser definidas: **ra√≠z** (stem), o morfema central da palavra, que supre o significado principal, e os **afixos**, que adicionam significados diversos.* [^4]

* **Vetores e Embeddings**

Para as tarefas acima, ainda s√£o utilizados os textos em forma de ***tokens***, em sua forma humanamente leg√≠vel por√©m com devidas normaliza√ß√µes de formas.

Para tarefas de processamento utilizando modelos computacionais √© preciso transformar os textos em dados num√©ricos, chamados **embeddings** , onde o "significado" dos ***tokens***, ao serem colocados em rela√ß√£o com os demais, recebe valores num√©ricos de acordo com sua distribui√ß√£o de probabilidade a partir de coocorr√™ncias. Nas famosas palavras do linguista J.R. Firth (1957), *‚ÄúYou shall know a word by the company it keeps‚Äù* , em tradu√ß√£o direta, "conheces uma palavra pelas companhias que mant√©m."

> *Modelos de Redes de Aprendizado Profundo, incluindo LLMs, s√£o incapazes de processar textos brutos diretamente. Visto que textos s√£o categ√≥ricos, n√£o s√£o compat√≠veis com as opera√ß√µes matem√°ticas usadas para implementar e treinar redes neurais. Portanto, se faz necess√°ria uma maneira de representar palavras como vetores de valores cont√≠nuos.* [^5]

Nos frameworks utilizados, SpaCy e Hugging Face ü§ó, os modelos s√£o carregados juntamente com um pipeline que adapta os pr√©-processamentos √†s necessidades de configura√ß√£o de cada modelo (a quantidade de dimens√µes de cada embedding, por exemplo, difere entre modelos). Portanto, todo texto processado pelos modelos neste projeto, passou por uma etapa de vetoriza√ß√£o camuflada.

### VIS√ÉO GERAL DO HUGGING FACE ü§ó

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

O Hugging Face ü§ó √© uma plataforma e reposit√≥rio de desenvolvimento e compartilhamento de modelos de IA. Dentro de seu ecossistema, existem diversas ferramentas e servi√ßos, e a maneira de intera√ß√£o com os modelos utilizados nesta etapa foi por meio do `pipeline`, uma maneira simples e intuitiva de utilizar transformers para tarefas pr√©-definidas, com poucas linhas de c√≥digo.

Cada pipeline envolve 3 subprocessos, j√° adaptados para extra√ß√£o de valores param√©tricos espec√≠ficos de cada modelo :

* **Pr√©-processamento** : transforma√ß√£o da string de input em embeddings conforme √† necessidade do modelo.
* **Processamento** : os dados de input s√£o processados pelo modelo.
* **P√≥s-processamento** : os resultados das predi√ß√µes s√£o transformados para facilitar a leitura dos valores obtidos.

> *[...] a abordagem moderna dominante para executar cada tarefa √© utilizar um √∫nico **modelo de base (foundation model)** e adapt√°-lo levemente utilizando quantidades relativamente pequenas de dados anotados, espec√≠ficos para cada tarefa [...] Esta se provou uma abordagem extremamente exitosa: para a grande maioria das tarefas [...], um modelo de base adaptado para uma tarefa supera vastamente os modelos anteriores ou os pipelines de modelos que foram criados para executar aquela tarefa em espec√≠fico.* [^6]

Para tarefas de NLP, alavancar resultados com base em grandes modelos de l√≠ngua pr√©-treinados √© a forma mais acess√≠vel e otimizada de trabalhar com datasets de texto. A abordagem adotada segue essa metodologia, os modelos utilizados foram treinados em cima de modelos de base, comentados na pr√≥xima se√ß√£o. A seguir, algumas das tarefas relevantes para o tipo de an√°lise deste projeto, dentre as diversas dispon√≠veis na plataforma Hugging Face ü§ó:

* `feature-extraction` obter a representa√ß√£o de um texto em vetores / embeddings
* `token-classification` obter classifica√ß√µes referentes a cada token (ex. POS tags)
* `ner` *named entity recognition*, √∫til para reconhecer "nomes pr√≥prios" referentes a regi√µes, institui√ß√µes, pessoas, marcas, etc
* `summarization` reduzir o tamanho de um texto sem perda de elementos semanticamente relevantes
* `text-classification` obter uma classifica√ß√£o, para a qual o modelo foi treinado, espec√≠fica para um determinado texto (ex. violento ou n√£o, detec√ß√£o de fake news, etc)
* `translation` tradu√ß√£o de textos
* `zero-shot-classification` classifica√ß√£o de texto sem categorias pr√©-definidas, identifica clusters sem√¢nticos e sugere categorias a partir deles, tamb√©m aceita categorias pr√©-definidas pelo usu√°rio

Para este projeto, foram utilizados 2 modelos da plataforma, treinados para tarefas espec√≠ficas de `text-classification` .

### MODELOS UTILIZADOS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A seguir os modelos utilizados nas plataformas utilizadas e algumas informa√ß√µes de refer√™ncia.

`explosion/en_core_web_trf` | SpaCy

[Modelo em ingl√™s](https://spacy.io/models/en#en_core_web_trf), treinado em `RoBERTa-base` utilizando textos da web (blogs, not√≠cias, coment√°rios), otimizado para tarefas de extra√ß√£o de vocabul√°rio, classifica√ß√£o de sintaxe e entidades.

`annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal` | Hugging Face ü§ó

[Modelo multil√≠ngue](https://huggingface.co/annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal) de detec√ß√£o de emo√ß√µes, misoginia e sexismo em discursos, o treino foi realizado em cima dos modelos-base e `XLM-RoBERTa`, `BERT` e `DistilBERT`, com *corpora* de coment√°rios em f√≥runs de discuss√£o de cunho pol√≠tico e datasets anotados. A seguir alguns dos m√©todos, crit√©rios e modelos complementares para obten√ß√£o de m√©tricas [^7]:

* **Detec√ß√£o de Toxicidade**
* **Detec√ß√£o de Emo√ß√µes**
* **Detec√ß√£o Multil√≠ngue de Misoginia e Sexismo**
  
`uget/sexual_content_dection` | Hugging Face ü§ó

[Modelo multil√≠ngue](https://huggingface.co/uget/sexual_content_dection) treinado com dataset anotado manualmente sobre o modelo-base `BERT-base-multilingual-cased` , especificamente para a classifica√ß√£o de texto, detectando conte√∫do sexual no texto, com fins de modera√ß√£o de conte√∫do p√∫blico.

## CAMADA TRUSTED ZONE: REVISANDO OS DADOS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Abaixo, ser√£o retomados os dados como est√£o dispostos na camada Trusted Zone, de onde ser√£o consumidos para as modelagens e ingress√£o posterior na Refined Zone. Os caminhos a seguir s√£o os utilizados como argumentos na execu√ß√£o do Job para input de dados.

* **Trusted Zone Local**

```bash
  s3://compass-desafio-final-dramance/Trusted/Local/Parquet/Movies/
```
  
![Trusted Zone Local](../evidencias/7-visao-trusted-filmes-local.png)

* **Trusted Zone TMDB**

```bash
  s3://compass-desafio-final-dramance/Trusted/TMDB/Parquet/Movies/2025/1/31/
```

![Trusted Zone TMDB](../evidencias/6-visao-trusted-filmes-tmdb-tabelas.png)

### PRIMEIRAS DESCOBERTAS NOS DADOS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para a an√°lise explorat√≥ria dos datasets e desenvolvimento dos c√≥digos para a modelagem dimensional foi utilizado um notebook no Databricks, disponibilizado no notebook [Dramance An√°lise Explorat√≥ria](./dramance_analise_exploratoria.ipynb) . A seguir, ser√£o comentados alguns pontos relevantes para as decis√µes de modelagem :

* **Conte√∫do Sexual N√£o-Filtrado**

A partir dos textos das colunas de t√≠tulos e sinopses, foi verificado que grande parte do conte√∫do do dataset possuia conte√∫do er√≥tico expl√≠cito, mesmo utilizando o filtro `include_adult=false` abaixo no endpoint de ingest√£o desses dados:

  ```python
    f"https://api.themoviedb.org/3/discover/movie?include_adult=false&\
                        include_video=false&language=en-US&page={pag}&\
                        primary_release_year={ano}&with_genres={genero_id}"
  ```

Pegando o `id 660041` de exemplo, verifica-se que o problema reside na pr√≥pria classifica√ß√£o interna do TMDB, pois ao ser consultado diretamente na base retorna-se o valor `adult: false` :

  |||
  |:---|---:|
  |![ID Conteudo Sexual](../evidencias/1-exploracao-dados-conteudo-sexual.png)|![ID Busca no TMDB](../evidencias/2-dados-conteudo-sexual-tmdb-false.png)|
  |||

Portanto, ser√£o inseridos filtros de classifica√ß√£o de texto para possibilitar a separa√ß√£o de tais dados na an√°lise, adicionando mais 2 valores a serem considerados `conteudo_sexual` e `sexismo` . Os modelos de l√≠ngua utilizados para essas etapas est√£o descritos na se√ß√£o [Modelos Utilizados](#modelos-utilizados).

* **Pa√≠ses Distintos | Categoriza√ß√£o de Regi√µes**

Foram localizados os pa√≠ses contidos no recorte analisado e, ent√£o, consideradas as classifica√ß√µes regionais a serem utilizadas para a coluna `regiao` da dimens√£o de Pa√≠ses.

![Pa√≠ses Distintos](../evidencias/4-exploracao-dados-paises-distintos.png)

* **Frequ√™ncia de L√≠nguas**

Inicialmente, foi considerado utilizar tradutores para obter uma vers√£o traduzida da coluna `titulo_original`, para entender discrep√¢ncias entre a ideia original e o `titulo_comercial` das obras.

No entanto, devido ao alto custo computacional, nota-se mais vantajoso um poss√≠vel recorte anal√≠tico dos t√≠tulos coreanos somente.

![Frequ√™ncia de L√≠nguas](../evidencias/5-exploracao-dados-freq-linguas.png)

> ‚ùó Essa extra√ß√£o da tradu√ß√£o ficou como uma implementa√ß√£o futura, e n√£o foi aplicada nesta etapa.

* **Lan√ßamentos por Ano**

Esta an√°lise √© fundamental para entender se a dissemina√ß√£o do acesso global √† plataforma Netflix (a partir de 2015-2016) pode indicar alguma rela√ß√£o com a quantidade de produ√ß√µes lan√ßadas, nas regi√µes de interesse no recorte da an√°lise.

![Lan√ßamos por Ano](../evidencias/40-exploracao-frequencia-filmes-ano.png)

### DIAGRAMA DA MODELAGEM DIMENSIONAL

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A seguir, o diagrama utilizado para guiar o desenvolvimento da modelagem dimensional:

[//]: # (Caso n√£o possua suporte para mermaid, sugiro abrir no site do GitHub para visualizar o diagrama a seguir ou instalar extens√£o compat√≠vel)

```mermaid
  erDiagram

  LINGUAS_DIM {
    int lingua_key
    str iso_cod
    str lingua
  }

  FILMES_VOCAB_BRIDGE {
    int filme_key
    int vocab_key
    int frequencia

  }

  VOCAB_DIM {
    int vocab_key
    str palavra
    str pos_class
  }

  ANALISE_TEXTUAL_DIM {
    int analise_textual_key
    str sinopse
    str texto_total
    int conteudo_sexual
    str sexismo
    str subs_mais_comum
    str verb_mais_comum
    str adj_mais_comum
    str adv_mais_comum
  }

  CORPORA_DIM ||--}|FILMES_FACT : catalogado_em
  TITULOS_DIM ||--|| FILMES_FACT : intitulado
  FILMES_FACT ||--|| ANALISE_TEXTUAL_DIM: analise_discursiva_em

      FILMES_FACT {
    int filme_key
    int tmdb_id
    str imdb_id
    int titulo_key
    int pais_key
    int lingua_key
    int analise_key
    int corpus_key
    int ano_lancamento
    float popularidade
    float media_avaliacao
    int qtd_avaliacoes
  }


  TITULOS_DIM {
    int titulo_key
    str titulo_comercial
    str titulo_original
  }

  PAISES_DIM {
    int pais_key
    str iso_cod
    str pais
    str regiao
  }

  CORPORA_DIM {
    int corpora_key
    str corpus
    date data_registro
  }

  FILMES_FACT }o--|| LINGUAS_DIM : falada_em
  PAISES_DIM ||--o{ FILMES_FACT: origem_em


  FILMES_FACT |{--}| FILMES_VOCAB_BRIDGE : contem_vocabulario
  FILMES_VOCAB_BRIDGE |{--}| VOCAB_DIM : termo_utilizado_em
```

Para as rela√ß√µes **muitos-para-muitos**, necess√°rias para organizar a frequ√™ncia de termos no vocabul√°rio de cada filme, foi utilizada uma **Tabela Bridge** ou **Tabela Associativa** que faz a conex√£o entre as tabelas fato `filmes_fact` e dimensional `vocab_dim` .

Para as rela√ß√µes de filmes com l√≠nguas, foram consideradas somente as l√≠nguas originais principais, de mesmo modo para o filme, foram desconsiderados pa√≠ses de produ√ß√£o adicionais. Assim, a rela√ß√£o dessas tabelas foi determinada como sendo de um (l√≠ngua / pa√≠s) para muitos filmes, e cada filme contendo 1 e somente 1 pa√≠s ou l√≠ngua original.

## DATA LAKE E CAMADA REFINED ZONE

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Arquitetura Data Lake AWS](../evidencias/34-arquitetura-aws.png)

### PROCESSAMENTO DE DADOS: ETAPA DE TRANSFORMA√á√ÉO PT. 2

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Nesta etapa da engenharia de dados do data lake, os datasets da camada Trusted Zone passam pela **modelagem dimensional**, extraindo informa√ß√µes e disponibilizando atributos de forma acess√≠vel para o consumo em tarefas anal√≠ticas. A ingress√£o das tabelas fato, dimens√µes e bridge ser√° feita na camada Refined Zone, e o processo todo ser√° executado em um Job no AWS Glue.

### AN√ÅLISE DO SCRIPT DO GLUE JOB

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Antes de executar o script, √© preciso instalar algumas depend√™ncias definidas em um arquivo `requirements.txt` :

```python
  torch
  spacy==3.7.2
  en_core_web_trf @ https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0.tar.gz
  transformers==4.35.0
```

Este arquivo est√° armazenado no bucket do data lake, e √© ingerido por meio de argumentos na [configura√ß√£o do job](#execu√ß√£o-do-glue-job).

#### IMPORTA√á√ïES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Script Importa√ß√µes](../evidencias/23-script-importacoes.png)

Al√©m do ambiente de execu√ß√£o j√° reproduzido em sprints anteriores, foram inclu√≠dos os seguintes trechos para os modelos de l√≠ngua utilizados.

* **Instala√ß√£o do modelo do SpaCy** : download do modelo a ser utilizado por meio de subprocesso.

```python
  try:
      subprocess.check_call([sys.executable, "-m", "spacy", "download", "en_core_web_trf"])
  except Exception as e:
      print(f"Erro de instala√ß√£o do modelo SpaCy: {e}")
```

* **Instala√ß√£o de pacotes de NLP**
  * `torch` PyTorch √© um pacote de Deep Learning que permite a utiliza√ß√£o do tipo Tensor para o processamento de texto nos modelos de transformers utilizados.
  * `re` Express√µes regulares s√£o utilizadas na limpeza e normaliza√ß√£o de texto customizada.
  * `transformers` e `spacy` : acesso aos modelos por meio de um pipeline de f√°cil acesso, com pr√©-processamento de texto adaptada aos par√¢metros de cada modelo, infer√™ncia baseada em tarefas e output simplificado.

```python
  # NLP
  import re
  from transformers import pipeline
  import spacy
  import torch
```

#### VARI√ÅVEIS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Abaixo a declara√ß√£o de vari√°veis de ambiente, as quais n√£o foram alteradas e seguem sendo as mesmas j√° detalhadas em outras etapas.
Somente ser√£o explicitadas as vari√°veis passadas como argumentos do sistema na configura√ß√£o do Job:

```python
  args["S3_TMDB_INPUT_PATH"] = "s3://compass-desafio-final-dramance/Trusted/TMDB/Parquet/Movies/2025/1/31"
  args["S3_LOCAL_INPUT_PATH"] = "s3://compass-desafio-final-dramance/Trusted/Local/Parquet/Movies"
  args["S3_BUCKET"] = "compass-desafio-final-dramance"
```

![Script Vari√°veis](../evidencias/22-script-variaveis.png)

#### MODELOS DE L√çNGUA

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Foram utilizados os 3 modelos de l√≠ngua, carregados em pipeline com os frameworks SpaCy e Hugging Face ü§ó, j√° detalhados anteriormente na se√ß√£o [Modelos Utilizados](#modelos-utilizados).

![Script Modelos de L√≠ngua](../evidencias/24-script-modelos-linguagem.png)

#### CRIA√á√ÉO DOS DATAFRAMES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Abaixo a cria√ß√£o dos DataFrames com os dados da Trusted Zone, j√° utilizando alguns *aliases* com `.alias()` para facilitar o acesso das colunas no JOIN seguinte.

![Cria√ß√£o dos DataFrames com Dados da Trusted Zone](../evidencias/21-script-dataframes-dados-trusted.png)

#### INTEGRA√á√ÉO DE DADOS ENTRE FONTES LOCAL E TMDB

Os DataFrames `filmes_local_df` e `filmes_tmdb_df` referem-se ao mesmo conjunto de dados, e s√£o complementares. Portanto, o *schema* ser√° definido e ambos ser√£o agregados em um DataFrame consolidado `filmes_df`.

![JOIN Filmes](../evidencias/20-script-join-filmes.png)

O processo √© realizado em 2 etapas:

1. Os dados de filmes de origem `Local` s√£o enriquecidos com os dados adicionais do `TMDB` . A estrutura de colunas entre os datasets fica assim:
  
  |coluna|local|tmdb|
  |:---|:---:|:---:|
  |tmdb_id||X|
  |imdb_id|X||
  |titulo_comercial|X||
  |titulo_original||X|
  |ano_lancamento|X||
  |lingua_original||X|
  |pais_origem||X|
  |popularidade||X|
  |media_avaliacao|X||
  |qtd_avaliacoes||X|
  |sinopse||X|

2. A tabela intermedi√°ria criada acima j√° adota o mesmo *schema* dos dados ingeridos do TMDB, portanto, pode ser consolidada com um comando UNION. Neste ponto, alguns filtros adicionais tamb√©m s√£o realizados, para precau√ß√£o de duplica√ß√µes.

#### FUN√á√ïES AUXILIARES, EXTRA√á√ÉO DE DADOS E CRIA√á√ÉO DE NOVAS COLUNAS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Abaixo, o processo de extra√ß√£o de valores adicionais a partir de infer√™ncias ou categoriza√ß√£o dos dados. Para a utiliza√ß√£o integrada ao ambiente Spark, os modelos de l√≠ngua s√£o acessados por meio de UDFs (*User Defined Functions*), utilizando um **decorator** na declara√ß√£o de fun√ß√µes.

##### CATEGORIZA√á√ÉO DE REGI√ïES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para esta classifica√ß√£o, foram criadas categorias regionais manualmente de acordo com o interesse geogr√°fico da an√°lise. Com o aux√≠lio do assistente de IA `Claude 3.5 Sonnet` , foi criado um dicion√°rio para a itera√ß√£o entre valores de pa√≠ses e regi√µes. No prompt foram fornecidos os valores da tabela `paises_df` com os c√≥digos ISO referentes aos pa√≠ses e as categorias a serem utilizadas.

![Prompt Claude](../evidencias/27-prompt-regioes-claude.png)

Considerando que os dados do dataset est√£o em ingl√™s, as categorias criadas para as regi√µes foram mantidas na mesma l√≠ngua. Somente um ajuste foi feito na cole√ß√£o gerada, substituindo somente o pa√≠s "M√©xico" da categoria "Norte Am√©rica" para "Caribe / Am√©rica Central", uma ado√ß√£o pertinente √† an√°lise a ser feita.

A itera√ß√£o sobre valores e localiza√ß√£o da regi√£o correspondente √© feita por meio da fun√ß√£o abaixo.

![Categoriza√ß√£o de Regi√µes](../evidencias/26-script-udf-regioes.png)

##### TEXTO TOTAL: T√çTULO + SINOPSE

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Como forma de aproveitar o conte√∫do sem√¢ntico presente na coluna `titulo_comercial` , foi criada uma coluna concatenando-a com a coluna `sinopse` , gerando a coluna `texto_total` , utilizada para as an√°lises de texto subsequentes.

![Coluna Texto Total](../evidencias/44-script-coluna-texto-total.png)

##### DETEC√á√ÉO DE CONTE√öDO SEXUAL E SEXISMO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Ambos os modelos utilizados, retornam um output bin√°rio (LABEL_0 ou LABEL_1) de classifica√ß√£o de texto com rela√ß√£o √† presen√ßa de conte√∫do sexual e discurso com tons sexistas e/ou mis√≥ginos, juntamente com o score de confian√ßa da infer√™ncia.

* `uget/sexual_content_dection` : modelo de classifica√ß√£o de conte√∫do sexual.
* `annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal` : modelo de classifica√ß√£o de presen√ßa de sexismo.

![Classifica√ß√£o sexismo e conte√∫do sexual](../evidencias/25-script-udf-sexual-sexismo.png)

Do formato de resultado a seguir, para as duas classifica√ß√µes foi extra√≠do somente o valor referente √† classifica√ß√£o - na chave `label`, o √∫ltimo d√≠gito, sendo 0 para negativo e 1 para positivo.

```json
  [{'label': 'LABEL_1', 'score': 0.999693751335144},
  {'label': 'LABEL_1', 'score': 0.9996908903121948},
  {'label': 'LABEL_0', 'score': 0.9864094257354736}]
```

Quanto √† performance desses modelos, foi registrado um caso de exemplo em que foi identificado corretamente tanto a presen√ßa de conte√∫do sexual quanto de sexismo, em um caso em que termos sugestivos n√£o estavam expl√≠citos no texto.

![Exemplo Classifica√ß√£o Conte√∫do Sexual e Sexismo](../evidencias/28-filtros-conteudo-sexual-sexismo.png)

##### CLASSES SINT√ÅTICAS E TERMOS COMUNS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Aqui, o modelo `en_core_web_trf` , carregado na vari√°vel `nlp` , √© utilizado como *parser* para extra√ß√£o de classes sint√°ticas lexicais, como substantivos (NOUN), verbos (VERB), adjetivos (ADJ) e adv√©rbios (ADV). E tamb√©m com a tarefa de lematiza√ß√£o dos termos, mantendo a sem√¢ntica de termos em flex√µes diferentes.

![Classifica√ß√£o Token POS](../evidencias/45-script-udf-pos-class.png)

O texto em `doc` √© tokenizado pelo modelo, assim √© poss√≠vel obter algumas informa√ß√µes a partir dos atributos de cada `token`.

```python
  token.lemma_      # Token em forma lematizada
  token.pos_        # Classe morfossint√°tica (POS Part-of-Speech)
  token.is_stop     # Se o token √© uma stopword
  token.is_alpha    # Se o token √© composto de caracteres alfab√©ticos
```

A partir da classe importada `Counter`, um contador gera uma cole√ß√£o de frequ√™ncia de termos a partir de uma string.

![Func Termo Mais Comum](../evidencias/46-script-func-termo-mais-freq.png)

Tais fun√ß√µes s√£o acessadas na cria√ß√£o das colunas abaixo, na tabela de `analise_textual` :

* **Colunas de lista de termos da categoria POS:** string (termos separados por v√≠rgulas)
  * substantivos
  * verbos
  * adjetivos
  * adverbios
* **Colunas de termos mais frequente das listas acima** : string
  * subst_mais_comum
  * verb_mais_comum
  * adj_mais_comum
  * adv_mais_comum

![Colunas Classe e Termos Frequentes](../evidencias/48-script-colunas-postags-termos-comuns.png)

No caso de n√£o existir um termo com frequ√™ncia acima dos demais, ser√° escolhido aleatoriamente um termo de valor mais alto.

##### FREQU√äNCIA DE TERMOS: DICIONARIZA√á√ÉO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para a integra√ß√£o de palavras normalizadas e classificadas em uma cole√ß√£o, foi criada uma fun√ß√£o que separa os tokens extra√≠dos, faz a limpeza de caracteres indesejados, e posteriormente insere esses termos em uma coluna `palavra` .

![Split Palavras Para Diciton√°rio](../evidencias/47-script-func-add-termo-vocab.png)

Essa fun√ß√£o foi utilizada no processo de cria√ß√£o das tabelas `vocab_dim` e `filmes_vocab_bridge` ela recebe os seguintes argumentos:

* `df` DataFrame com a coluna de termos a ser inclu√≠da em uma cole√ß√£o ou dicion√°rio de termos
* `coluna_palavras` coluna com a lista de termos, uma string com tokens separados
* `padrao_process` o padr√£o em RegEx para o tratamento de separa√ß√£o de termos da lista
  * `\\[|\\]` o padr√£o declarado seleciona colchetes para remo√ß√£o
* `sep` o separador entre os tokens da lista
* `padrao_add_vocab` o padr√£o em RegEx para tratamento ap√≥s o split, precedendo a inser√ß√£o do termo
  * `^\\s+|\\s+$` o padr√£o declarado seleciona espa√ßos em branco no come√ßo e fim da string
* `coluna_extra` coluna de refer√™ncia (geralmente um ID), a ser selecionada e inclu√≠da no DataFrame resultante

![Coluna Frequ√™ncia](../evidencias/50-script-coluna-frequencia.png)

#### CRIA√á√ÉO DAS TABELAS DIMENS√ÉO, FATO E BRIDGE

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Nesta se√ß√£o, as colunas criadas anteriormente e as tabelas auxiliares utilizadas nesse processo s√£o consolidadas na vers√£o final das tabelas ap√≥s a modelagem dimensional.

Para a cria√ß√£o de chaves prim√°rias, foi utilizada a fun√ß√£o `monotonically_increasing_id()` com o acr√©scimo do inteiro `1` para que a indexa√ß√£o n√£o comece do valor 0 (zero). Os IDs gerados s√£o crescentes e √∫nicos, por√©m n√£o s√£o necessariamente consecutivos, ainda assim, esse fator n√£o prejudica a modelagem. 

E, para os caso de rela√ß√µes entre tabelas que possuem colunas de IDs compartilhados, foi utilizada a fun√ß√£o `alias()` para facilitar a referencia√ß√£o nos JOINs seguintes.

##### CRIA√á√ÉO DIMENS√ÉO L√çNGUAS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para a dimens√£o L√≠nguas, foi utilizada a tabela `linguas_df` do TMDB como base, algumas c√©lulas multivaloradas tiveram seus valores tratados.

* `lingua_key`
* `iso_cod`
* `lingua`

![Script Dimens√£o L√≠nguas](../evidencias/18-script-dimensao-linguas.png)

##### CRIA√á√ÉO DIMENS√ÉO PA√çSES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A dimens√£o Pa√≠ses √© criada ap√≥s a extra√ß√£o da categoria `regiao` com a fun√ß√£o auxiliar `obter_regiao` aplicada ao DataFrame `paises_df`, tamb√©m com dados obtidos do TMDB.

* `pais_key`
* `iso_cod`
* `pais`
* `regiao`

![Script Dimens√£o Pa√≠ses](../evidencias/19-script-dimensao-paises.png)

##### CRIA√á√ÉO DIMENS√ÉO T√çTULOS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para a dimens√£o de T√≠tulos, al√©m das colunas de interesse, foi inclu√≠do o `tmdb_id` para facilitar na rela√ß√£o da dimens√£o com a tabela fato, visto que os dados de t√≠tulo n√£o s√£o confi√°veis como identificador √∫nico de cada filme.

* `titulo_key`
* `titulo_comercial`
* `titulo_original`

![Script Dimens√£o T√≠tulos](../evidencias/51-script-dimensao-titulos.png)

##### CRIA√á√ÉO DIMENS√ÉO AN√ÅLISE TEXTUAL

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Na dimens√£o de An√°lise Textual, todas as infer√™ncias e extra√ß√µes de classifica√ß√£o de texto e tokens s√£o selecionadas, e tamb√©m √© utilizado `tmdb_id` para auxiliar na rela√ß√£o posterior. Em alguns casos, como este, a sele√ß√£o √© feita em 2 etapas, primeiro com a ordena√ß√£o dos valores, para depois ser gerada a chave prim√°ria. Aqui, a an√°lise √© referente a cada filme individualmente.

* `analise_key`
* `sinopse`
* `texto_total`
* `conteudo_sexual`
* `sexismo`
* `subst_mais_comum`
* `verbo_mais_comum`
* `adj_mais_comum`
* `adv_mais_comum`

![Script Dimens√£o An√°lise](../evidencias/52-script-dimensao-analise.png)

##### CRIA√á√ÉO DIMENS√ÉO CORPORA

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A dimens√£o Corpora busca centralizar todos os textos de um *corpus* para uma an√°lise espec√≠fica, podendo explicitar tend√™ncias que permeiam os exemplares como um todo. Ou seja, inclui todos os textos analisados na dimens√£o An√°lise Textual. Neste caso, tamb√©m pareceu √∫til incluir o timestamp de cria√ß√£o, para an√°lises comparativas com outros *corpus* ou vers√µes do mesmo *corpus*.

* `corpus_key`
* `corpus`
* `data_registro`

![Script Dimens√£o Corpora](../evidencias/53-script-dimensao-corpora.png)

##### CRIA√á√ÉO DIMENS√ÉO VOCABUL√ÅRIO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Para a cria√ß√£o da dimens√£o Vocabul√°rio, com ocorr√™ncias distintas de tokens j√° normalizados e categorizados, √© utilizado como base o DataFrame `analise_textual_df` , o qual possui os termos extra√≠dos da coluna `texto_total` separados por classe sint√°tica.

O processo √© realizado em 2 etapas:

* **Cria√ß√£o de DataFrame intermedi√°rio**: a partir da fun√ß√£o `stack`, a qual transforma colunas em linhas. Portanto, os arrays de termos contidos nas colunas `substantivos` , `verbos` , `adjetivos` e `adverbios` s√£o translocados para linhas individuais de um DataFrame de termos e classes.

* **Split de termos e limpeza pr√©-inser√ß√£o**: a partir da fun√ß√£o definida no in√≠cio do script `split_palavras_vocab` , as colunas com array de termos s√£o tratadas linha a linha com 2 padr√µes distintos, definidos pelo usu√°rio (bem como o separador a se considerar), os padr√µes definidos foram:
  * `padrao_process = \\[|\\]` remo√ß√£o de colchetes [ e ]
  * `padrao_add_vocab = ^\\s+|\\s+$` remove espa√ßos vazios no in√≠cio ou fim do token  

![Script Dimens√£o Vocabul√°rio](../evidencias/54-script-dimensao-vocab.png)

Ap√≥s isso, a dimens√£o Vocabul√°rio √© criada, filtrando linhas vazias e valores nulos que possam ter passado nas etapas anteriores, e ordenada por classe sint√°tica.

##### CRIA√á√ÉO FATO FILMES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A cria√ß√£o da tabela fato Filmes foi feita em 2 etapas, somente para contornar um problema de conflito de IDs n√£o resolvido apesar dos aliases criados, separando os JOINs em blocos. Para as dimens√µes que n√£o possuiam um c√≥digo √∫nico antes de sua cria√ß√£o, foi utilizado o `tmdb_id` como auxiliar.

![Script Fato Filmes Pt. 1](../evidencias/55-script-fato-parte1.png)

O JOIN da tabela de An√°lise Textual foi feito separadamente.

* `filme_key`
* `tmdb_id`
* `imdb_id`
* `titulo_key`
* `pais_key`
* `lingua_key`
* `corpus_key`
* `ano_lancamento`
* `popularidade`
* `media_avaliacao`
* `qtd_avaliacoes`

![Script Fato Filmes Pt. 2](../evidencias/56-script-fato-parte2.png)

##### CRIA√á√ÉO BRIDGE FILMES-VOCABUL√ÅRIO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

O DataFrame intermedi√°rio `contagem_palavras` gerado [nesta etapa](#frequ√™ncia-de-termos-dicionariza√ß√£o), com a coluna de `frequencia` de termos por filme, √© utilizado como base para a tabela associativa / bridge Filmes-Vocabul√°rio.

As palavras s√£o relacionadas com a dimens√£o Vocabul√°rio por meio da coluna `palavra` (correspondente ao termo lematizado, sendo substantivo, adjetivo, verbo ou adv√©rbio) e com a fato Filmes por meio do `tmdb_id`, incluso no DataFrame como auxiliar.

* `filme_key`
* `vocab_key`
* `frequencia`

![Script Bridge Filmes Vocab](../evidencias/57-script-bridge-filmes-linguas.png)

##### DELE√á√ÉO DE COLUNAS DE ID AUXILIARES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Como j√° comentado acima, em alguns processos de cria√ß√£o das tabelas, foi preciso utilizar um ID comum entre os dados para estabelecer as rela√ß√µes em uma etapa intermedi√°ria, antes da cria√ß√£o das novas chaves prim√°rias/estrangeiras para cada nova tabela.

Nesta etapa, ap√≥s estabelecidas as rela√ß√µes com as novas chaves, essas colunas s√£o exclu√≠das das tabelas em que n√£o s√£o pertinentes. No caso presente, foi utilizada somente a coluna `tmdb_id` para os 3 casos necess√°rios.

![Dele√ß√£o de Colunas Auxiliares](../evidencias/58-script-delecao-colunas-id-aux.png)

#### INGRESS√ÉO NA REFINED ZONE

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

A ingress√£o de dados na Refined Zone segue o mesmo padr√£o para todas tabelas criadas na se√ß√£o anterior, portanto, s√≥ ser√° explicitado o bloco de ingress√£o da tabela que precede a conclus√£o do script e o commit do Job.

![Script Ingress√£o Refined Zone](../evidencias/59-script-ultima-ingressao-job-commit.png)

Aqui a tabela √© consolidada em um √∫nico arquivo com `coalesce(1)` , salva em formato `parquet` no caminho declarado na vari√°vel `s3_refined_output`, a execu√ß√£o √© conclu√≠da com a listagem do conte√∫do do bucket, com o arquivo de logs fechado e enviado ao bucket com a data atual. A seguir, retomam-se os valores das vari√°veis de output para consulta:

```python
  s3_refined_output = f"s3://{nome_bucket}/Refined"
  log = f"log-transform-refined-{ano}{mes}{dia}"
```

## EXECU√á√ÉO DO GLUE JOB

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

√â preciso realizar algumas instala√ß√µes de pacotes para a execu√ß√£o do script, registrados no arquivo `requirements.txt` , e passando os seguintes argumentos na configura√ß√£o do Job:

```bash
  KEY: --python-modules-installer-option VALUE: -r
  KEY: --additional-python-modules VALUE: s3://bucket/caminho/para/requirements.txt
```

![Job Argumentos](../evidencias/16-job-parameters.png)

O arquivo [requirements.txt](requirements.txt) foi armazenado no bucket no seguinte caminho:

```bash
  s3://compass-desafio-final-dramance/Config/requirements.txt
```

![Requirements Bucket](../evidencias/17-job-requirements.png)

Foi necess√°rio aumentar a capacidade e quantidade de *workers*, ap√≥s erros de espa√ßo em disco, e os tempos de execu√ß√£o excediam 30min. Esses fatores ser√£o comentados mais a frente, na se√ß√£o [Dificuldade e Pontos de Melhoria](#dificuldades-e-pontos-de-melhoria).

![Execu√ß√£o Glue Job](../evidencias/42-execucao-job-refined.gif)

> ‚ùó A cria√ß√£o e execu√ß√£o do Crawler acima n√£o √© a vers√£o com a cria√ß√£o da tabela Bridge Filmes-Vocab. Devido √†s dificuldades com a limita√ß√£o de testes no Glue, tanto o Job quanto o Crawler final que gera essa tabela e se refere ao script `job_refined.py` n√£o foram registrados em v√≠deo.

### CRIA√á√ÉO E EXECU√á√ÉO DO CRAWLER

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Finalizada a execu√ß√£o dos Glue Jobs, e gerados os arquivos na Refined Zone, √© poss√≠vel extrair seu schema e metadados, catalogando-os automaticamente em tabelas com um Crawler.

![Cria√ß√£o Crawler e Tables](../evidencias/43-execucao-crawler.gif)

Abaixo, os comandos utilizados para as verifica√ß√µes de databases e tabelas criados, durante o v√≠deo de apresenta√ß√£o:

```bash
  # Listando databases
  aws glue get-databases

  # Listando tabelas
  aws glue get-tables --database-name dramance_db
```

> ‚ùó A cria√ß√£o e execu√ß√£o do Crawler acima n√£o √© a vers√£o com a cria√ß√£o da tabela Bridge Filmes-Vocab. Devido √†s dificuldades com a limita√ß√£o de testes no Glue, tanto o Job quanto o Crawler final que gera essa tabela e se refere ao script `job_refined.py` n√£o foram registrados em v√≠deo.

![Tabelas Crawler](../evidencias/41-glue-tables.png)

## MODELAGEM DIMENSIONAL: VIS√ÉO GERAL COM ATHENA

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

Ap√≥s a execu√ß√£o do Crawler, √© poss√≠vel confirmar a correta modelagem dos dados, a partir de uma amostra de cada tabela por meio do AWS Athena com queries em SQL:

```sql
  SELECT * FROM "AwsDataCatalog"."dramance_db"."filmes_fact" limit 10;
```

### TABELA DIMENSIONAL L√çNGUAS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o L√≠nguas](../evidencias/8-dimensional-linguas-athena.png)

### TABELA DIMENSIONAL PA√çSES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o Pa√≠ses](../evidencias/9-dimensional-paises-athena.png)

### TABELA DIMENSIONAL T√çTULOS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o T√≠tulos](../evidencias/10-dimensional-titulos-athena.png)

### TABELA DIMENSIONAL AN√ÅLISE TEXTUAL

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o An√°lise Textual](../evidencias/11-dimensional-analise-textual-athena.png)

### TABELA DIMENSIONAL VOCABUL√ÅRIO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o Vocabul√°rio](../evidencias/12-dimensional-vocabulario-athena.png)

### TABELA DIMENSIONAL CORPORA

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Dimens√£o Corpora](../evidencias/31-dimensional-corpora-athena.png)

### TABELA FATO FILMES

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Fato Filmes](../evidencias/13-fato-filmes-athena.png)

### TABELA BRIDGE FILMES-VOCABUL√ÅRIO

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

![Bridge Filmes Vocab](../evidencias/60-filmes-vocab-bridge-athena.png)

## VIS√ÉO GERAL DO BUCKET DRAMANCE

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

* **"Ra√≠z" do Bucket**

![Vis√£o Geral Bucket](../evidencias/37-visao-geral-bucket.png)

* **Refined Zone**

![Refined Zone](../evidencias/35-bucket-refined-zone.png)

* **Refined Zone: Tabela-Fato Filmes**

![Refined Zone Tabela Fato Filmes](../evidencias/36-bucket-refined-zone-filmes-fact.png)

* **Logs de Execu√ß√£o**

![Logs de Execu√ß√£o](../evidencias/38-bucket-logs.png)

## CONSIDERA√á√ïES FINAIS: DIFICULDADES E PONTOS DE MELHORIA

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

O processo de desenvolvimento e testes com modelos de l√≠ngua, somado ao alto custo de execu√ß√£o por erros no Glue, foi moroso e esteve sujeito a modifica√ß√µes devido ao longo tempo para experimenta√ß√£o com modelos e designs relacionais entre as tabelas.

Todo o projeto de modelagem e teste de infer√™ncias nos dados foi realizado em ambiente Databricks, com 15 GB RAM dispon√≠vel em cluster Spark, e as tabelas demoravam mais de 10 min para conclus√£o das transforma√ß√µes.

![Demora Execu√ß√£o no Databricks](../evidencias/32-tempo-computacao-exemplo-fimes-vocab-bridge.png)

Nos primeiros testes no Glue para certifica√ß√£o de que as depend√™ncias estavam sendo instaladas de acordo, ap√≥s alguns ajustes em runs mais curtas, foi lan√ßado o erro:

![Erro No Space](../evidencias/30-erro-execucao-no-space-disk.png)

Foi preciso aumentar a configura√ß√£o: primeiro no n¬∫ de *workers*; n√£o sendo suficiente, aumentou-se a capacidade e o n¬∫ de *workers*. Nas runs subsequentes, a configura√ß√£o foi sendo ajustada buscando o m√≠nimo poss√≠vel e, por isso, a quantidade de testes no Glue tamb√©m foi limitada.

A 1¬™ execu√ß√£o de sucesso foi para o teste de instala√ß√µes e download dos modelos, e cria√ß√£o da tabela `linguas_dim`, a qual n√£o cont√©m nenhuma infer√™ncia com modelos.

A 2¬™ execu√ß√£o de sucesso, a mais recente, foi a execu√ß√£o completa da modelagem das tabelas e ingress√£o na Refined Zone. Com exce√ß√£o da tabela `filmes_vocab_bridge` , que n√£o constava no script desse Job. O √∫ltimo Job, contendo a tabela `filmes_vocab_bridge` durou cerca de 12 minutos a mais, somente para a gera√ß√£o dessa tabela.

![√öltimo Job](../evidencias/61-ultima-run.png)

Para otimizar o fluxo, seria preciso melhorar o planejamento da capacidade de computa√ß√£o necess√°ria, mapear a quantidade de c√°lculos e infer√™ncias necess√°rias para o c√≥digo, entender qual esse custo de acordo com as especificidades de cada modelo e mapear poss√≠veis duplica√ß√µes de computa√ß√µes no script.

A seguir, alguns estudos iniciais de mapeamento de custos:

* **Tamanho do Modelo | N¬∫ de Par√¢metros**
  * **en_core_web_trf** : 463 MB | 125 M par√¢metros
  * **xlm-roberta-base-misogyny-sexism-indomain-mix-bal** : 1.11 GB | 279 M par√¢metros
  * **sexual_content_dection** : 711 MB | 178 M par√¢metros

* **Quantidade de Infer√™ncias para Cada Modelo**
  * **en_core_web_trf** : 4 execu√ß√µes x 484 linhas = 1936 infer√™ncias
  * **annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal** : 1 execu√ß√£o x 484 linhas = 484 infer√™ncias
  * **sexual_content_dection** : 1 execu√ß√£o x 484 linhas = 484 infer√™ncias

* **Custo Job AWS Glue** : cobran√ßa por segundo rodado
  * **1 DPU/hora** : 0.44 USD
  * **Configura√ß√£o** : 4 DPUs
  * **Tempo de execu√ß√£o**: 45m 25s = 0.757 horas
  * **Custo total**: 0.757 √ó 4 √ó $0.44 = 1.33 USD

## REFER√äNCIAS

*Voltar para **Se√ß√µes*** [÷ç](#se√ß√µes)

[^1]: JURAFSKY, MARTIN; 2025, p. 23
[^2]: ZHENG, CASARI; 2018, p. 52
[^3]: Ibidem, p. 48
[^4]: JURAFSKY, MARTIN; 2025, p. 23
[^5]: RASCHKA, 2025, p. 18
[^6]: PAPADIMITRIOU, MANNING; 2021, p. 23
[^7]: RONG-CHING, MAY, LERMAN; 2023, p. 88